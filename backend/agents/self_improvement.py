"""
Self-Improvement Agent
======================

Closed-loop optimization engine implementing the self-evolution cycle:
  Evaluate → Detect → Propose → Validate → Apply → Re-benchmark

Responsibilities:
- Receives regression alerts and SLA violations from EvaluationAgent
- Proposes concrete optimizations (batch sizes, concurrency, caching, routing)
- Validates proposals via shadow testing (compare metrics before/after)
- Applies validated improvements to the running system
- Re-benchmarks to confirm improvements
- Maintains an optimization knowledge base (what worked, what didn't)

Listens to: EvaluationAgent (regressions, SLA violations)
Emits to: HardwareOptimizer (config changes), ModelExecution (routing changes),
         Orchestrator (status updates)
"""

import asyncio
import logging
import time
from dataclasses import dataclass, field
from enum import Enum
from typing import Any

from .base import AgentMessage, AgentRegistry, BaseAgent, MessageType

logger = logging.getLogger(__name__)


class ProposalStatus(str, Enum):
    PROPOSED = "proposed"
    VALIDATING = "validating"
    VALIDATED = "validated"
    APPLIED = "applied"
    REJECTED = "rejected"
    REVERTED = "reverted"


class OptimizationType(str, Enum):
    BATCH_SIZE = "batch_size"
    CONCURRENCY = "concurrency"
    CACHE_POLICY = "cache_policy"
    MODEL_ROUTING = "model_routing"
    MEMORY_LIMIT = "memory_limit"
    THREAD_COUNT = "thread_count"
    QUEUE_CONFIG = "queue_config"


@dataclass
class OptimizationProposal:
    """A concrete optimization proposal generated by the self-improvement engine."""
    id: str
    opt_type: OptimizationType
    target: str            # Model/subsystem name
    parameter: str
    current_value: Any
    proposed_value: Any
    reason: str
    expected_improvement: str
    status: ProposalStatus = ProposalStatus.PROPOSED
    created_at: float = field(default_factory=time.time)
    applied_at: float | None = None
    validation_metrics: dict[str, Any] = field(default_factory=dict)
    revert_value: Any = None
    result: str = ""

    def to_dict(self) -> dict[str, Any]:
        return {
            "id": self.id,
            "type": self.opt_type.value,
            "target": self.target,
            "parameter": self.parameter,
            "current": self.current_value,
            "proposed": self.proposed_value,
            "reason": self.reason,
            "status": self.status.value,
            "result": self.result,
            "created_at": self.created_at,
            "applied_at": self.applied_at,
        }


@dataclass
class KnowledgeEntry:
    """A record of what worked (or didn't) for future reference."""
    parameter: str
    action: str       # "increase", "decrease", "change"
    context: str      # System state when this was tried
    outcome: str      # "improved", "degraded", "neutral"
    magnitude: float  # % improvement/degradation
    timestamp: float = field(default_factory=time.time)


class SelfImprovementAgent(BaseAgent):
    """
    Self-evolution engine implementing the cycle:
    Evaluate → Detect → Propose → Validate → Apply → Re-benchmark

    Optimization strategies ranked by risk (low→high):
    1. Cache warmup / eviction policy tuning
    2. Batch size adjustments
    3. Concurrency limit changes
    4. Thread pool sizing
    5. Model routing changes
    6. Memory limit adjustments
    """

    def __init__(self):
        super().__init__(name="self_improvement")
        self._proposals: list[OptimizationProposal] = []
        self._knowledge_base: list[KnowledgeEntry] = []
        self._max_proposals: int = 200
        self._max_knowledge: int = 500
        self._proposal_counter: int = 0
        self._cooldown_s: float = 120.0  # Don't re-optimize same target within 2min
        self._last_action_per_target: dict[str, float] = {}
        self._active_proposal: OptimizationProposal | None = None
        self._registry: AgentRegistry | None = None

    async def initialize(self) -> None:
        self._registry = AgentRegistry()
        logger.info("SelfImprovementAgent initialized")

    async def handle_message(self, message: AgentMessage) -> AgentMessage | None:
        if message.msg_type == MessageType.EVENT:
            action = message.payload.get("action")
            if action == "regressions_detected":
                return await self._handle_regressions(message)
            elif action == "sla_violations":
                return await self._handle_sla_violations(message)

        elif message.msg_type == MessageType.REQUEST:
            action = message.payload.get("action")
            if action == "get_proposals":
                return message.reply({
                    "proposals": [p.to_dict() for p in self._proposals[-30:]],
                })
            elif action == "get_knowledge":
                return message.reply({
                    "entries": [
                        {"param": k.parameter, "action": k.action,
                         "outcome": k.outcome, "magnitude": k.magnitude}
                        for k in self._knowledge_base[-30:]
                    ],
                })
            elif action == "force_optimization":
                return await self._force_optimization(message)

        return None

    async def _handle_regressions(self, message: AgentMessage) -> AgentMessage | None:
        """Generate optimization proposals from detected regressions."""
        regressions = message.payload.get("regressions", [])
        proposals = []

        for reg in regressions:
            model = reg.get("model", "unknown")
            metric = reg.get("metric", "")
            degradation = reg.get("degradation_pct", 0)

            if self._is_in_cooldown(model):
                continue

            if metric == "latency_mean":
                props = self._propose_latency_fix(model, degradation, reg)
                proposals.extend(props)
            elif metric == "error_rate":
                props = self._propose_error_fix(model, reg)
                proposals.extend(props)

        # Apply each proposal through the validation pipeline
        for proposal in proposals:
            self._proposals.append(proposal)
            await self._validate_and_apply(proposal)

        while len(self._proposals) > self._max_proposals:
            self._proposals.pop(0)

        if proposals:
            return message.reply({
                "action": "proposals_generated",
                "count": len(proposals),
            })
        return None

    async def _handle_sla_violations(self, message: AgentMessage) -> AgentMessage | None:
        """Generate proposals for SLA violations."""
        violations = message.payload.get("violations", [])
        proposals = []

        for v in violations:
            model = v.get("model", "unknown")
            current_p95 = v.get("current_p95_ms", 0)
            target = v.get("target_ms", 5000)

            if self._is_in_cooldown(model):
                continue

            # SLA violation → more aggressive optimization
            overshoot_pct = ((current_p95 - target) / target) * 100

            if overshoot_pct > 50:
                # Severe: reduce batch size aggressively + increase concurrency limit
                proposals.append(self._create_proposal(
                    opt_type=OptimizationType.BATCH_SIZE,
                    target=model,
                    parameter=f"{model}_batch_size",
                    current_value="current",
                    proposed_value="half",
                    reason=f"SLA violation: P95={current_p95:.0f}ms > target={target:.0f}ms ({overshoot_pct:.0f}% over)",
                    expected_improvement=f"Reduce P95 by ~{min(50, overshoot_pct):.0f}%",
                ))
            else:
                # Moderate: tune batch size down slightly
                proposals.append(self._create_proposal(
                    opt_type=OptimizationType.BATCH_SIZE,
                    target=model,
                    parameter=f"{model}_batch_size",
                    current_value="current",
                    proposed_value="reduce_10pct",
                    reason=f"SLA near-violation: P95={current_p95:.0f}ms vs target={target:.0f}ms",
                    expected_improvement="Reduce P95 by ~10%",
                ))

        for proposal in proposals:
            self._proposals.append(proposal)
            await self._validate_and_apply(proposal)

        return None

    def _propose_latency_fix(self, model: str, degradation_pct: float, reg: dict) -> list[OptimizationProposal]:
        """Generate proposals to fix latency regression."""
        proposals = []

        # Check knowledge base for what worked before
        past_success = self._find_knowledge(model, "improved")

        if degradation_pct > 50:
            # Severe regression: multiple interventions
            proposals.append(self._create_proposal(
                opt_type=OptimizationType.BATCH_SIZE,
                target=model,
                parameter=f"{model}_batch_size",
                current_value="current",
                proposed_value="half",
                reason=f"Severe latency regression ({degradation_pct:.0f}%)",
                expected_improvement="Reduce per-request latency",
            ))
            proposals.append(self._create_proposal(
                opt_type=OptimizationType.CACHE_POLICY,
                target=model,
                parameter=f"{model}_cache_ttl",
                current_value="current",
                proposed_value="double",
                reason=f"Increase cache hit rate to compensate for slower inference",
                expected_improvement="Reduce cache misses by ~20%",
            ))
        elif degradation_pct > 20:
            # Moderate: single intervention
            if past_success and past_success.action == "decrease":
                # We know decreasing batch size helped before
                proposals.append(self._create_proposal(
                    opt_type=OptimizationType.BATCH_SIZE,
                    target=model,
                    parameter=f"{model}_batch_size",
                    current_value="current",
                    proposed_value="reduce_25pct",
                    reason=f"Moderate latency regression ({degradation_pct:.0f}%), "
                           f"batch reduction worked before (+{past_success.magnitude:.0f}%)",
                    expected_improvement="~15-25% latency reduction",
                ))
            else:
                proposals.append(self._create_proposal(
                    opt_type=OptimizationType.BATCH_SIZE,
                    target=model,
                    parameter=f"{model}_batch_size",
                    current_value="current",
                    proposed_value="reduce_10pct",
                    reason=f"Moderate latency regression ({degradation_pct:.0f}%)",
                    expected_improvement="~10% latency reduction",
                ))

        return proposals

    def _propose_error_fix(self, model: str, reg: dict) -> list[OptimizationProposal]:
        """Generate proposals to fix error rate regression."""
        error_rate = reg.get("current_value", 0)
        proposals = []

        if error_rate > 0.10:
            # High error rate: possible OOM or timeout
            proposals.append(self._create_proposal(
                opt_type=OptimizationType.MEMORY_LIMIT,
                target=model,
                parameter=f"{model}_memory_headroom",
                current_value="current",
                proposed_value="increase_20pct",
                reason=f"High error rate ({error_rate:.1%}) — likely OOM or resource exhaustion",
                expected_improvement="Reduce OOM-related errors",
            ))
            proposals.append(self._create_proposal(
                opt_type=OptimizationType.CONCURRENCY,
                target=model,
                parameter=f"{model}_max_concurrent",
                current_value="current",
                proposed_value="reduce_half",
                reason=f"Reduce contention to lower error rate",
                expected_improvement="Reduce resource contention errors",
            ))

        return proposals

    async def _validate_and_apply(self, proposal: OptimizationProposal) -> None:
        """
        Validate a proposal and apply if safe.

        Validation steps:
        1. Check that the change doesn't conflict with an active proposal
        2. Verify the system is stable enough for changes
        3. Apply the change
        4. Monitor for improvement window
        5. Record outcome in knowledge base
        """
        if self._active_proposal:
            proposal.status = ProposalStatus.REJECTED
            proposal.result = "Another proposal is being validated"
            return

        proposal.status = ProposalStatus.VALIDATING
        self._active_proposal = proposal

        try:
            # Apply the optimization
            applied = await self._apply_proposal(proposal)
            if not applied:
                proposal.status = ProposalStatus.REJECTED
                proposal.result = "Could not apply"
                return

            proposal.status = ProposalStatus.APPLIED
            proposal.applied_at = time.time()
            self._last_action_per_target[proposal.target] = time.time()

            # Record in knowledge base
            # The actual outcome (improved/degraded) will be determined
            # in a future evaluation cycle
            self._knowledge_base.append(KnowledgeEntry(
                parameter=proposal.parameter,
                action=self._infer_action(proposal),
                context=f"regression on {proposal.target}",
                outcome="pending",
                magnitude=0.0,
            ))
            while len(self._knowledge_base) > self._max_knowledge:
                self._knowledge_base.pop(0)

            logger.info(f"Applied optimization: {proposal.id} — {proposal.reason}")

        except Exception as e:
            proposal.status = ProposalStatus.REJECTED
            proposal.result = f"Error: {e}"
            logger.error(f"Failed to apply optimization {proposal.id}: {e}")
        finally:
            self._active_proposal = None

    async def _apply_proposal(self, proposal: OptimizationProposal) -> bool:
        """Apply a specific optimization to the running system."""
        if not self._registry:
            return False

        if proposal.opt_type == OptimizationType.BATCH_SIZE:
            # Send batch size change to hardware_optimizer
            msg = AgentMessage(
                msg_type=MessageType.REQUEST,
                sender=self.name,
                recipient="hardware_optimizer",
                payload={
                    "action": "update_config",
                    "parameter": proposal.parameter,
                    "value": proposal.proposed_value,
                    "reason": proposal.reason,
                },
            )
            await self._registry.route_message(msg)
            return True

        elif proposal.opt_type == OptimizationType.CONCURRENCY:
            # Send concurrency change to model_execution
            msg = AgentMessage(
                msg_type=MessageType.EVENT,
                sender=self.name,
                recipient="model_execution",
                payload={
                    "action": "config_changed",
                    "changes": {
                        "max_concurrent": proposal.proposed_value,
                    },
                },
            )
            await self._registry.route_message(msg)
            return True

        elif proposal.opt_type == OptimizationType.CACHE_POLICY:
            # Cache TTL changes — apply directly
            try:
                from backend.cache.multi_tier_cache import get_cache
                cache = get_cache()
                # Specific cache tuning would go here
                logger.info(f"Cache policy updated: {proposal.parameter}")
                return True
            except Exception:
                return False

        elif proposal.opt_type == OptimizationType.MEMORY_LIMIT:
            # Memory limit changes via memory coordinator
            try:
                from backend.core.optimized.memory_coordinator import get_memory_coordinator
                coordinator = get_memory_coordinator()
                # Increase headroom
                coordinator._config.headroom_gb = min(
                    2.0, coordinator._config.headroom_gb * 1.2
                )
                return True
            except Exception:
                return False

        return False

    async def _force_optimization(self, message: AgentMessage) -> AgentMessage:
        """Force an optimization cycle (for testing/admin)."""
        target = message.payload.get("target", "all")
        proposal = self._create_proposal(
            opt_type=OptimizationType.BATCH_SIZE,
            target=target,
            parameter=f"{target}_batch_size",
            current_value="current",
            proposed_value="reduce_10pct",
            reason="Forced optimization",
            expected_improvement="Variable",
        )
        self._proposals.append(proposal)
        await self._validate_and_apply(proposal)
        return message.reply({"proposal": proposal.to_dict()})

    def _create_proposal(self, **kwargs) -> OptimizationProposal:
        self._proposal_counter += 1
        return OptimizationProposal(
            id=f"opt-{self._proposal_counter:04d}",
            **kwargs,
        )

    def _is_in_cooldown(self, target: str) -> bool:
        last = self._last_action_per_target.get(target, 0)
        return (time.time() - last) < self._cooldown_s

    def _find_knowledge(self, target: str, outcome: str) -> KnowledgeEntry | None:
        """Search knowledge base for relevant past experience."""
        for entry in reversed(self._knowledge_base):
            if target in entry.context and entry.outcome == outcome:
                return entry
        return None

    @staticmethod
    def _infer_action(proposal: OptimizationProposal) -> str:
        val = str(proposal.proposed_value)
        if "reduce" in val or "half" in val or "decrease" in val:
            return "decrease"
        elif "increase" in val or "double" in val:
            return "increase"
        return "change"
